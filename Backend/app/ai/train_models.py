# train_models.py
import asyncio
import json
import os
from pathlib import Path
from datetime import datetime, timedelta

import numpy as np
import pandas as pd
from dotenv import load_dotenv

from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker
from sqlalchemy import text


def load_env() -> Path | None:
    """
    ì‹¤í–‰ ìœ„ì¹˜ê°€ ì–´ë””ë“  .envë¥¼ ì¡ê¸° ìœ„í•´:
    - í˜„ì¬ íŒŒì¼ í´ë”
    - ìƒìœ„ í´ë”ë“¤(ìµœëŒ€ 4ë‹¨ê³„)
    ë¥¼ ìˆœíšŒí•˜ë©° íƒìƒ‰
    """
    here = Path(__file__).resolve().parent
    candidates = [
        here / ".env"
    ]

    for p in candidates:
        if p.exists():
            # override=Trueë¡œ .env ê°’ì´ í™•ì‹¤íˆ ì ìš©ë˜ê²Œ(í™˜ê²½ë³€ìˆ˜ ì¶©ëŒ ë°©ì§€)
            load_dotenv(dotenv_path=p, override=True)
            return p
    return None


def get_db_url() -> str:
    env_path = load_env()
    db_url = os.getenv("DATABASE_URL")

    if not db_url:
        raise SystemExit(
            "âŒ DATABASE_URLì´ ë¹„ì–´ìˆìŒ.\n"
            f"   - .env ìœ„ì¹˜: {env_path if env_path else 'ì°¾ì§€ ëª»í•¨'}\n"
            "   - .envì— DATABASE_URL=postgresql+asyncpg://... ê°€ ìˆëŠ”ì§€ í™•ì¸í•´."
        )
    return db_url


def otsu_threshold(values: np.ndarray, nbins: int = 128) -> float:
    """ì¥ë¹„ë³„ energy_amp ë¶„í¬ì—ì„œ STANDBY/LOAD ì„ê³„ê°’ ìë™ ì¶”ì •(Otsu)."""
    values = values[np.isfinite(values)]
    if len(values) < 50:
        return float(np.nan)

    vmin, vmax = float(values.min()), float(values.max())
    if vmax <= vmin:
        return float(np.nan)

    hist, bin_edges = np.histogram(values, bins=nbins, range=(vmin, vmax))
    hist = hist.astype(np.float64)

    prob = hist / (hist.sum() + 1e-12)
    omega = np.cumsum(prob)
    mu = np.cumsum(prob * (bin_edges[:-1] + bin_edges[1:]) / 2.0)
    mu_t = mu[-1]

    sigma_b2 = (mu_t * omega - mu) ** 2 / (omega * (1.0 - omega) + 1e-12)
    idx = int(np.nanargmax(sigma_b2))
    thr = (bin_edges[idx] + bin_edges[idx + 1]) / 2.0
    return float(thr)


def robust_baseline(values: np.ndarray):
    """Median + MAD ê¸°ë°˜ baseline(ì´ìƒì¹˜ íƒì§€ìš©)."""
    values = values[np.isfinite(values)]
    if len(values) < 30:
        return None
    med = float(np.median(values))
    mad = float(np.median(np.abs(values - med)))
    return {"median": med, "mad": mad}


async def fetch_devices(days: int = 14) -> pd.DataFrame:
    db_url = get_db_url()
    start_ts = datetime.now() - timedelta(days=days)

    print(f"ğŸ“Œ fetch_devices() start | days={days} | start_ts={start_ts}", flush=True)

    # âœ… ì—°ê²°/ì¿¼ë¦¬ íƒ€ì„ì•„ì›ƒ(ë©ˆì¶¤ ë°©ì§€)
    # - timeout: ì»¤ë„¥ì…˜ íƒ€ì„ì•„ì›ƒ(ì´ˆ)
    # - command_timeout: ì¿¼ë¦¬ ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ(ì´ˆ)
    engine = create_async_engine(
        db_url,
        echo=False,
        pool_pre_ping=True,
        connect_args={"timeout": 10, "command_timeout": 60},
    )
    SessionLocal = async_sessionmaker(engine, expire_on_commit=False)

    sql = """
    SELECT device_mac, device_name, temperature, humidity, energy_amp, relay_status, "timestamp"
    FROM public.devices
    WHERE "timestamp" >= :start_ts
    ORDER BY "timestamp" ASC
    """

    try:
        async with SessionLocal() as session:
            res = await session.execute(text(sql), {"start_ts": start_ts})
            rows = res.fetchall()
            cols = res.keys()
    finally:
        await engine.dispose()

    df = pd.DataFrame(rows, columns=cols)
    print(f"âœ… fetch_devices() done | rows={len(df)}", flush=True)
    return df


async def main():
    print("ğŸš€ train_models.py start", flush=True)

    # í˜¹ì‹œë¼ë„ â€œì¤‘ê°„ì— ë©ˆì¶¤â€ì´ë©´ ì—¬ê¸°ì„œ ê±¸ë¦¼ â†’ ë¡œê·¸ë¡œ í™•ì¸ ê°€ëŠ¥
    df = await fetch_devices(days=14)

    if df.empty:
        raise SystemExit("âŒ í•™ìŠµí•  ë°ì´í„°ê°€ ì—†ìŒ(ìµœê·¼ 14ì¼).")

    print(f"ğŸ“Š raw df shape = {df.shape}", flush=True)

    # íƒ€ì… ì •ë¦¬
    df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce")
    df["relay_status"] = df["relay_status"].astype(str).str.lower()

    thresholds = {}
    baselines = {}

    unique_devices = df["device_mac"].nunique(dropna=True)
    print(f"ğŸ” unique device_mac = {unique_devices}", flush=True)

    for mac, g in df.groupby("device_mac"):
        g_on = g[g["relay_status"] == "on"].copy()
        amps = g_on["energy_amp"].astype(float).to_numpy()

        thr = otsu_threshold(amps)
        if not np.isfinite(thr):
            thr = float(np.nanmedian(amps)) if len(amps) else 0.05

        device_name = str(g["device_name"].iloc[-1]) if len(g) else ""

        thresholds[mac] = {
            "device_name": device_name,
            "standby_load_threshold_amp": float(thr),
        }

        base = robust_baseline(amps)
        if base:
            baselines[mac] = {
                "device_name": device_name,
                "amp_median": base["median"],
                "amp_mad": base["mad"],
            }

    out_dir = Path(__file__).resolve().parent / "models"
    out_dir.mkdir(parents=True, exist_ok=True)

    (out_dir / "thresholds.json").write_text(
        json.dumps(thresholds, ensure_ascii=False, indent=2),
        encoding="utf-8"
    )
    (out_dir / "baselines.json").write_text(
        json.dumps(baselines, ensure_ascii=False, indent=2),
        encoding="utf-8"
    )

    print(f"âœ… saved thresholds: {out_dir / 'thresholds.json'} ({len(thresholds)} devices)", flush=True)
    print(f"âœ… saved baselines : {out_dir / 'baselines.json'} ({len(baselines)} devices)", flush=True)
    print("ğŸ‰ train_models.py finished", flush=True)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except Exception as e:
        # ì–´ë–¤ ì´ìœ ë¡œë“  â€œì¡°ìš©íˆ ì¢…ë£Œâ€í•˜ëŠ” ê±¸ ë§‰ê³ , ì—ëŸ¬ë¥¼ í™•ì‹¤íˆ ë³´ì´ê²Œ
        import traceback
        print("ğŸ’¥ ERROR ë°œìƒ:", repr(e), flush=True)
        traceback.print_exc()
        raise
